import argparse
import os
import pickle
import sys

import numpy as np
import torch, json
from torch.nn import BCELoss
from torch.optim import Adam

from data_loader.dataset import DataSet
from modules.model import DevignModel, GGNNSum
from trainer import train
from utils import tally_param, debug
from tqdm import tqdm, trange

def save_embed(model, num_batches, data_iter, examples, filename):
    model.eval()
    debug(f'Saving to {filename}')

    with torch.no_grad():
        embeds = torch.zeros((0, 200), dtype=float)
        for _ in trange(num_batches, desc='> Generating'):
            graph, _ = data_iter()
            embeds = torch.cat((embeds, model(graph, cuda=True, save=True).detach().cpu()))

        start = 0
        max_nodes = max(x.num_nodes for x in examples)
        debug(f"Embedding: {embeds.size()}")
        with open(filename, "w") as f:
            for sample in tqdm(examples, desc=f'Writing to {filename}'):
                num = sample.num_nodes
                end = start + num
                print(json.dumps({'node_features': embeds[start:end].tolist(), 'graph': sample.edges, 'targets': sample.target, 'max': max_nodes}), file=f)
                start = end

        assert end == embeds.size(0), f'Size mismatch: {embeds.size(0)} & {end}'

if __name__ == '__main__':
    torch.manual_seed(1000)
    np.random.seed(1000)
    parser = argparse.ArgumentParser()
    parser.add_argument('--model_type', type=str, help='Type of the model (devign/ggnn)',
                        choices=['devign', 'ggnn'], default='devign')
    parser.add_argument('--input_dir', type=str, help='Input Directory of the parser', default='input')
    parser.add_argument('--node_tag', type=str, help='Name of the node feature.', default='node_features')
    parser.add_argument('--graph_tag', type=str, help='Name of the graph feature.', default='graph')
    parser.add_argument('--label_tag', type=str, help='Name of the label feature.', default='targets')

    parser.add_argument('--feature_size', type=int, help='Size of feature vector for each node', default=100)
    parser.add_argument('--graph_embed_size', type=int, help='Size of the Graph Embedding', default=200)
    parser.add_argument('--num_steps', type=int, help='Number of steps in GGNN', default=6)
    parser.add_argument('--batch_size', type=int, help='Batch Size for training', default=128)

    parser.add_argument('--no_train', action="store_true")
    parser.add_argument("--gpu", type=str, required=True)
    args = parser.parse_args()

    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu

    if args.feature_size > args.graph_embed_size:
        print('Warning!!! Graph Embed dimension should be at least equal to the feature dimension.\n'
              'Setting graph embedding size to feature size', file=sys.stderr)
        args.graph_embed_size = args.feature_size

    model_dir = 'models'
    if not os.path.exists(model_dir):
        os.makedirs(model_dir)
    input_dir = args.input_dir
    processed_data_path = os.path.join(input_dir, 'processed.bin')
    if os.path.exists(processed_data_path):
        debug('Reading already processed data from %s!' % processed_data_path)
        dataset = pickle.load(open(processed_data_path, 'rb'))
        debug(len(dataset.train_examples), len(dataset.valid_examples), len(dataset.test_examples))
    else:
        dataset = DataSet(train_src=os.path.join(input_dir, 'train_GGNNinput.json'),
                          valid_src=os.path.join(input_dir, 'valid_GGNNinput.json'),
                          test_src=os.path.join(input_dir, 'test_GGNNinput.json'),
                          batch_size=args.batch_size, n_ident=args.node_tag, g_ident=args.graph_tag,
                          l_ident=args.label_tag)
        file = open(processed_data_path, 'wb')
        pickle.dump(dataset, file)
        file.close()
    assert args.feature_size == dataset.feature_size, \
        f'Dataset contains different feature vector {dataset.feature_size} than argument feature size {args.feature_size}. ' \
        'Either change the feature vector size in argument, or provide different dataset.'
    if args.model_type == 'ggnn':
        model = GGNNSum(input_dim=dataset.feature_size, output_dim=args.graph_embed_size,
                        num_steps=args.num_steps, max_edge_types=dataset.max_edge_type)
    else:
        model = DevignModel(input_dim=dataset.feature_size, output_dim=args.graph_embed_size,
                            num_steps=args.num_steps, max_edge_types=dataset.max_edge_type)

    debug('Total Parameters : %d' % tally_param(model))
    debug('#' * 100)
    model.cuda() #! May crash here on first launch. Be careful.
    loss_function = BCELoss(reduction='sum')
    optim = Adam(model.parameters(), lr=0.0001, weight_decay=0.001)

    if args.no_train:
        model.load_state_dict(torch.load(model_dir + '/GGNNSumModel-model.bin'))
        DIR = 'saved_embeds'
        os.makedirs(DIR, exist_ok=True)
        save_embed(model, dataset.initialize_test_batch(batch_size=128), dataset.get_next_test_batch, dataset.test_examples, f'{DIR}/test.json')
        save_embed(model, dataset.initialize_valid_batch(batch_size=128), dataset.get_next_valid_batch, dataset.valid_examples, f'{DIR}/valid.json')
        save_embed(model, dataset.initialize_train_batch(batch_size=128), dataset.get_next_train_batch, dataset.train_examples, f'{DIR}/train.json')
    else:
        train(model=model, dataset=dataset, max_steps=(len(dataset.train_examples) + 127) // 128 * 100, dev_every=128,
            loss_function=loss_function, optimizer=optim,
            save_path=model_dir + '/GGNNSumModel', max_patience=1000000, log_every=None)
