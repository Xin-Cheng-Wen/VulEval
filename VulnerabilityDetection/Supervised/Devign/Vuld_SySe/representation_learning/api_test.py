import argparse
import json
import numpy
import os
import sys
import torch
from representation_learning_api import RepresentationLearningModel
from sklearn.model_selection import train_test_split
from baseline_svm import SVMLearningAPI
from tqdm import tqdm, trange

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--features', default='ggnn', choices=['ggnn', 'wo_ggnn'])
    parser.add_argument('--lambda1', default=0.5, type=float)
    parser.add_argument('--lambda2', default=0.001, type=float)
    parser.add_argument('--baseline', action='store_true')
    parser.add_argument('--baseline_balance', action='store_true')
    parser.add_argument('--baseline_model', default='svm')
    parser.add_argument('--num_layers', default=1, type=int)
    parser.add_argument("--gpu", type=str, required=True)
    parser.add_argument('--no_train', action="store_true")
    numpy.random.rand(1000)
    torch.manual_seed(1000)
    args = parser.parse_args()
    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu
    dataset = 'proj'
    ds = '../../_devign/saved_embeds/'
    feature_name = args.features
    parts = ['train', 'valid', 'test']
    assert isinstance(dataset, str)
    output_dir = 'results_test'
    if args.baseline:
        output_dir = 'baseline_' + args.baseline_model
        if args.baseline_balance:
            output_dir += '_balance'

    if not os.path.exists(output_dir):
        os.mkdir(output_dir)
    output_file_name = output_dir + '/' + dataset.replace('/', '_') + '-' + feature_name + '-'
    output_file_name = 'no_train.log' if args.no_train else 'train.log'
    output_file = open(output_file_name, 'w')
    features = {'train': [], 'valid': [], 'test': []}
    targets = {'train': [], 'valid': [], 'test': []}

    for part in parts:
        filename = ds + part + ".json"
        with open(filename) as json_data_file:
            data = [json.loads(line) for line in tqdm(json_data_file, desc=f'Reading {filename}')]
        for d in data:
            features[part].append(numpy.sum(d['node_features'], axis=0)) #! Reveal simply sums over node embeddings as aggregiation
            targets[part].append(d['targets'])
        del data
    print(f'Result: {len(features["train"])}x{len(features["train"][0])}, {len(features["valid"])}x{len(features["valid"][0])}, {len(features["test"])}x{len(features["test"][0])}')
    X, Y = features, targets
    print('=' * 100, file=sys.stderr, flush=True)
    all_results = {'accuracy': 0.0, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0}
    for i in trange(5, desc='>>> Overall process'):
        if args.baseline:
            print(">>> SVMLearningAPI")
            model = SVMLearningAPI(True, args.baseline_balance, model_type=args.baseline_model)
        else:
            print(">>> RepresentationLearningModel")
            model = RepresentationLearningModel(
                lambda1=args.lambda1, lambda2=args.lambda2, batch_size=128, print=True, max_patience=5, balance=True,
                num_layers=args.num_layers
            )
        if not args.no_train:
            model.train(X['train'], Y['train'], X['valid'], Y['valid'], 200, i)
        results = model.evaluate(X['test'], Y['test'], i)
        for item in results:
            all_results[item] += results[item] / 5
        s = f"Acc: {results['accuracy']:.3f}, Precision: {results['precision']:.3f}, Recall: {results['recall']:.3f}, F1: {results['f1']:.3f}"
        print(s, flush=True, file=output_file)
        print(s)
        print('-' * 150)

    results = all_results
    s = f"(Overall) Acc: {results['accuracy']:.3f}, Precision: {results['precision']:.3f}, Recall: {results['recall']:.3f}, F1: {results['f1']:.3f}"
    print(s, flush=True, file=output_file)
    print(s)
    print('-' * 150)
    output_file.close()
