# random
  python run.py \
      --output_dir=./saved_models/train_1 \
      --model_type=roberta \
      --tokenizer_name=microsoft/codebert-base \
      --model_name_or_path=microsoft/codebert-base \
      --do_train_1\
      --labels_file=./data/data_result/ \
      --label_ratio  1 \
      --train_data_file dataset/all/random/train_c_cpp.jsonl \
      --eval_data_file dataset/all/random/valid_c_cpp.jsonl \
      --test_data_file dataset/all/random/test_c_cpp.jsonl \
      --epoch 5 \
      --block_size 512 \
      --train_batch_size 32 \
      --eval_batch_size 64 \
      --learning_rate 2e-5 \
      --max_grad_norm 1.0 \
      --evaluate_during_training \
      --seed 123456  2>&1 | tee train1_random.log 

  python run.py \
      --output_dir=./saved_models/train_1 \
      --model_type=roberta \
      --tokenizer_name=microsoft/codebert-base \
      --model_name_or_path=microsoft/codebert-base \
      --do_step1\
      --labels_file=./data/data_result/ \
      --label_ratio  1 \
      --train_data_file dataset/all/random/train_c_cpp.jsonl \
      --eval_data_file dataset/all/random/valid_c_cpp.jsonl \
      --test_data_file dataset/all/random/test_c_cpp.jsonl \
      --epoch 5 \
      --P_num 5532 \
      --N_num 180259 \
      --block_size 512 \
      --train_batch_size 32 \
      --eval_batch_size 64 \
      --learning_rate 2e-5 \
      --max_grad_norm 1.0 \
      --evaluate_during_training \
      --seed 123456  2>&1 | tee step1_random.log 

  python run.py \
      --output_dir=./saved_models/train_2 \
      --model_type=roberta \
      --tokenizer_name=microsoft/codebert-base \
      --model_name_or_path=microsoft/codebert-base \
      --do_train_2\
      --labels_file=./data/data_result/ \
      --label_ratio  1 \
      --train_data_file dataset/all/random/train_c_cpp.jsonl \
      --eval_data_file dataset/all/random/valid_c_cpp.jsonl \
      --test_data_file dataset/all/random/test_c_cpp.jsonl \
      --epoch 5 \
      --block_size 512 \
      --train_batch_size 32 \
      --eval_batch_size 64 \
      --learning_rate 2e-5 \
      --max_grad_norm 1.0 \
      --evaluate_during_training \
      --seed 123456  2>&1 | tee train2_random.log 

      python run.py \
      --output_dir=./saved_models/train_2 \
      --model_type=roberta \
      --tokenizer_name=microsoft/codebert-base \
      --model_name_or_path=microsoft/codebert-base \
      --do_train_iterative\
      --labels_file=./data/data_result/ \
      --label_ratio  1 \
      --train_data_file dataset/all/random/train_c_cpp.jsonl \
      --eval_data_file dataset/all/random/valid_c_cpp.jsonl \
      --test_data_file dataset/all/random/test_c_cpp.jsonl \
      --epoch 5 \
      --P_num 5532 \
      --N_num 180259 \
      --block_size 512 \
      --train_batch_size 32 \
      --eval_batch_size 64 \
      --learning_rate 2e-5 \
      --max_grad_norm 1.0 \
      --evaluate_during_training \
      --seed 123456  2>&1 | tee train_iterative_random.log

      python run.py \
      --output_dir=./saved_models/train_3 \
      --model_type=roberta \
      --tokenizer_name=microsoft/codebert-base \
      --model_name_or_path=microsoft/codebert-base \
      --do_train_3\
      --labels_file=./data/data_result/ \
      --label_ratio  1 \
      --train_data_file dataset/all/random/train_c_cpp.jsonl \
      --eval_data_file dataset/all/random/valid_c_cpp.jsonl \
      --test_data_file dataset/all/random/test_c_cpp.jsonl \
      --epoch 5 \
      --block_size 512 \
      --train_batch_size 32 \
      --eval_batch_size 64 \
      --learning_rate 2e-5 \
      --max_grad_norm 1.0 \
      --evaluate_during_training \
      --seed 123456  2>&1 | tee train3_random.log