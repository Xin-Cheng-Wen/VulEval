import openai
import jsonlines
import json
import sys
from tqdm import tqdm
import traceback
from time import sleep
import tiktoken

api_keys = ["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",
"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",
"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",
"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",
"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",
"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",
"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",
"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",
"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",
"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",
"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",
"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",
"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",
"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",
"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",
"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",
"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",
"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",
"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",
"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",
"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",
"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",
"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",
"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",
"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",
"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",
"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",
"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",
"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",
"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",
"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",
"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",
"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",
"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",
"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",
"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",
"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",
"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",
"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",
"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",
"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",
"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",
"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",
"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",
"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",
"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",
"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",
"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",
"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",
"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",
"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",
"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",
"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",
"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",
"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",
"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",
"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",
"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",
"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",
"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",
"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",
"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",
"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",
"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",
"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",
#"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",
#"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",
#"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",
#"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",
#"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",
]

number = 1
# int(sys.argv[1])
total = 1
# int(sys.argv[2])

# int(sys.argv[3])
filename = './test_c_cpp_repository2.jsonl'
    # sys.argv[1]

# openai.organization = "org-feTtcnydCkPSmIW2aJBSGgI0"
querys = []
with jsonlines.open(filename) as reader:
    for obj in reader:
        querys.append(obj)
querys = querys[int((number-1)*(len(querys)/total)):int((number)*(len(querys)/total))]

results = []
fail = []
'''
numbers = []
with open('idx.txt', 'r') as file:
    lines = file.readlines()

    for line in lines:
        numbers.append(int(line.strip()))
#print(numbers)
'''


def count_and_slice_tokens(text, callee_of_change, caller_of_change, prompt, encoding):
    # print(len(text))
    max_length = 1024
    count_code = len(encoding.encode(text))
    count_prompt = len(encoding.encode(prompt))

    encode_code = encoding.encode(text)



    # 截取前1000个token
    if count_code + count_prompt > max_length:
        truncated_tokens = encode_code[:max_length - count_prompt]
        truncated_text = encoding.decode(encode_code[:max_length - count_prompt])

        output = prompt + '\n' + truncated_text

        # truncated_text = " ".join(truncated_tokens)
        return output
    elif caller_of_change != {} or callee_of_change != {}:
        new_list = ["//Reference Code:\n"]
        for key, value in callee_of_change.items():
            formatted_value = []


            for line in value.split('\n'):
                if line.strip():  # Check if the line is not empty after stripping whitespace
                    formatted_value.append('// ' + line)
            formatted_value.append('\n'.join(formatted_value))
            formatted_value = ('\n'.join(formatted_value))



            new_list.append(f"// {key} : \n {formatted_value}")
        for key, value in caller_of_change.items():
            formatted_value = []
            for line in value.split('\n'):
                if line.strip():  # Check if the line is not empty after stripping whitespace
                    formatted_value.append('// ' + line)
            formatted_value.append('\n'.join(formatted_value))
            formatted_value = ('\n'.join(formatted_value))

            new_list.append(f"// {key}: {formatted_value}")

        add_comment = '\n'.join(new_list)

        if count_code < max_length:
            left_len = max_length - count_code - count_prompt
            text = prompt + encoding.decode(encoding.encode(add_comment)[:left_len]) + '\n' + text
        return text
    else:
        return prompt + '\n' + text

encoding = tiktoken.get_encoding("cl100k_base")
# token_count = 0
for pos in tqdm(range(len(querys))):
    api_idx = pos % len(api_keys)
    openai.api_key = api_keys[api_idx]
    '''
    if pos not in numbers:
        continue
    '''
    query = querys[pos]


    success = 0
    fail_count = 0

    prompt = 'You are an expert C and C++ vulnerability detector. The vulnerabilities such as Improper Restriction of Operations within the Bounds of a Memory Buffer, Out-of-bounds Read, Out-of-bounds Write, NULL Pointer Dereference, Improper Input Validation and so on.'
    prompt_1 = 'Decide whether the following code snippet is vulnerable or not?  You can only answer vulnerable or not. If yes, it means the code snippet is vulnerable. If no, it means the code snippet is non-vulnerable.\\n          '

    # code = query['func'].replace('\n', '\\\\n')
    code = query['function']

    caller_of_change = query['caller_of_change']
    callee_of_change = query['caller_of_change']
    text = count_and_slice_tokens(code, caller_of_change, callee_of_change, prompt_1, encoding)




    target = query['target']
    # print(target)



    while success != 1:
        try:
        # 解析 API 响应以获取 token 数量


        #
        # token_count += count
        # if pos % 10 == 0:
        #     print("Pos:"+str(pos) +" Count" + str(token_count))
            response = openai.Completion.create(
                model="gpt-3.5-turbo-instruct",
                prompt= text,
                max_tokens=2048
            )
            success=1
            result = {}
            result['pos'] = pos
            result['target'] = query['target']
            result['choices'] = response.get("choices")[0]["text"]
            result['func'] = text

            # print(result['choices'])
            # print(result['func'])
            # exit(0)

            # int((number-1)*(len(querys))) + pos
            with jsonlines.open(filename.split('.jsonl')[0]+'_Instruct_RQ3_golden_3.jsonl', mode='a') as f:
                f.write_all([result])
        except Exception as e:
            print("Error:" + str(pos))
            print(e)
            sleep(5)
            fail_count+=1

        if fail_count>5:
            fail.append(pos)
            break
        sleep(5)
        # success = 1

print(fail)


