# -*- coding: utf-8 -*-
import time
import json
from vllm import LLM, SamplingParams
import re
import os

def write_jsonl(filename, data):
    with open(filename, "w") as fp:
        for x in data:
            fp.write((json.dumps(x, ensure_ascii=False) + "\n"))

def read_jsonl(filename):
    data = []
    with open(filename) as f:
        for line in f:
            data.append(json.loads(line))
    return data

def generate(llm, sampling_params, file_path, batch_size, write_path):

    problems = read_jsonl(file_path)
    print(f'{file_path} total: {len(problems)}')
    start = time.time()
    num = 0
    fim = False
    prompt = 'You are an expert C/C++ programmer. '
    prompt_1 = 'Decide whether a code snippet is vulnerable or not?  You can only answer vulnerable or not. If yes, it means the code snippet contains a vulnerable function. If no, it means the code snippet doesn\'t contain a vulnerable function.\\n '

    if fim:
        prompts = ["<fim-prefix>" + problem['prompt'].strip() + "<fim-suffix><fim-middle>" for problem in problems]
    else:
        prompts = [prompt + problem['function'].strip() + prompt_1 for problem in problems]

    vllm_input = [prompts[i:i+batch_size] for i in range(0, len(prompts), batch_size)]
    input_token_num = 0
    output_token_num = 0
    i = 0
    result = []
    with open(write_path, 'w') as f:
        for item in vllm_input:
            infer_start = time.time()
            outputs = llm.generate(item, sampling_params)
            for _, output in enumerate(outputs):
                text = output.outputs[0].text
                obj = {'function_id': problems[i]['function_id'], 'target': problems[i]['target'], 'answer': text}
                f.write((json.dumps(obj, ensure_ascii=False) + "\n"))
                i += 1
                

def main():
    os.environ["CUDA_VISIBLE_DEVICES"] = "2,3"

    model_path = "/data/xcwen/ReposVulBench/llama/CodeLlama-13b-Instruct-hf"
    tokenizer_path = model_path
    parallel_size = 2     # gpu数量
    # file_path = '/data/xcwen/ReposVulBench/llama/dataset/all/random/test_c_cpp.jsonl'    # 数据集
    # write_path = '/data/xcwen/ReposVulBench/llama/dataset/all/random/test_c_cpp_answer_codellama_13b.jsonl'
    file_path = '/data/xcwen/ReposVulBench/llama/dataset/all/time/test_c_cpp.jsonl'    # 数据集
    write_path = '/data/xcwen/ReposVulBench/llama/dataset/all/time/test_c_cpp_answer_codellama_13b.jsonl'
    
    batch_size = 8     # batch大小
    sampling_params = SamplingParams(temperature=0.2, top_p=0.95, max_tokens=512)
    llm = LLM(model=model_path, tensor_parallel_size=parallel_size, tokenizer=tokenizer_path,trust_remote_code=True, gpu_memory_utilization=0.6)
    print('load model success')
    generate(llm, sampling_params, file_path, batch_size, write_path)
    

if __name__ == '__main__':
    main()